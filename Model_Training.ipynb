{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcbw2BNSnu86"
   },
   "source": [
    "## **Project: Conversational Next Word Predictor**\n",
    "Project Type: NLP / Generative AI / LSTM Dataset: DailyDialog (Kaggle)\n",
    "\n",
    "### **Project Overview :**\n",
    "This project focuses on building a Deep Learning model capable of understanding and predicting natural human conversation. Unlike standard text generators trained on Wikipedia (which are formal and encyclopedic), this model is trained on the DailyDialog dataset to capture the flow, tone, and grammar of casual English dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1764134883063,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "oC__o9WzBPcF",
    "outputId": "1bfd1cd9-30a2-4174-bf1e-bd5abc8f9413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version : 2.19.0\n"
     ]
    }
   ],
   "source": [
    "#Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(f'Tensorflow Version : {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6Gg7KzRpPnS"
   },
   "source": [
    "### **1. Data Loading (Pre-processed)**\n",
    "The model is trained on the **DailyDialog** dataset. To ensure high-quality input, the raw data underwent a rigorous **external data engineering process** before being loaded here.\n",
    "\n",
    "**The Pre-processing Pipeline involved:**\n",
    "1.  **Parsing:** Converting stringified lists from the raw CSV into flat text.\n",
    "2.  **Sanitization:** Removing artifacts like brackets `['...']` and non-English punctuation (e.g., Chinese full stops).\n",
    "3.  **Sentence Splitting:** Fixing \"fused\" sentences (e.g., \"How are you?I am fine\") using Regex.\n",
    "\n",
    "The resulting clean dataset is loaded from `final_training_data_refined.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1764126205090,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "Qo2zNHkMBgPu",
    "outputId": "a5d93c62-e52a-4498-bb8a-66e522027fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded 168870 conversation lines.\n",
      "Sample : Say , Jim , how about going for a few beers after dinner ?\n"
     ]
    }
   ],
   "source": [
    "#Loading the cleaned data\n",
    "with open (\"Data/final_training_data_refined.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "#Removing empty strings if any\n",
    "text_data = [line for line in lines if len(line) > 1]\n",
    "\n",
    "print(f'Succesfully loaded {len(text_data)} conversation lines.')\n",
    "print(f'Sample : {text_data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kta_GX5pv7I"
   },
   "source": [
    "### **2. Tokenization & Sequence Generation**\n",
    "We convert text into sequences of integers.\n",
    "\n",
    "- **Tokenizer:** Fits on the corpus to build a dictionary of the top 15,000 words.\n",
    "\n",
    "- **Persistence:** The tokenizer is serialized using pickle so the exact same mapping can be used in the deployment app.\n",
    "\n",
    "- **N-Grams:** We use a sliding window approach to generate multiple training examples from a single sentence (e.g., \"Hi how\" -> \"are\", \"Hi how are\" -> \"you\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1638,
     "status": "ok",
     "timestamp": 1764126212380,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "VWqvZ7JRBhyM",
    "outputId": "12d1492c-9fc1-4a48-b657-ee1f7b57c551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Words :  19391\n"
     ]
    }
   ],
   "source": [
    "## Tokenization (converting words to numbers)\n",
    "VOCAB_SIZE = 15000\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Total Unique Words : \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1764131307617,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "FLJlVu-EVU5w"
   },
   "outputs": [],
   "source": [
    "#Save Tokenizer\n",
    "with open('models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3596,
     "status": "ok",
     "timestamp": 1764126220221,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "sqYJtD5nBuRW"
   },
   "outputs": [],
   "source": [
    "## Creating N-Grams (Input Sequences)\n",
    "input_sequences = list()\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1764126230923,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "u-NdLKF5BxLs",
    "outputId": "62c33998-6cff-45ae-a217-5c145f976534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151, 974] --> ['say', 'jim']\n",
      "[151, 974, 31] --> ['say', 'jim', 'how']\n",
      "[151, 974, 31, 34] --> ['say', 'jim', 'how', 'about']\n",
      "[151, 974, 31, 34, 75] --> ['say', 'jim', 'how', 'about', 'going']\n",
      "[151, 974, 31, 34, 75, 12] --> ['say', 'jim', 'how', 'about', 'going', 'for']\n",
      "[151, 974, 31, 34, 75, 12, 5] --> ['say', 'jim', 'how', 'about', 'going', 'for', 'a']\n",
      "[151, 974, 31, 34, 75, 12, 5, 199] --> ['say', 'jim', 'how', 'about', 'going', 'for', 'a', 'few']\n",
      "[151, 974, 31, 34, 75, 12, 5, 199, 3257] --> ['say', 'jim', 'how', 'about', 'going', 'for', 'a', 'few', 'beers']\n",
      "[151, 974, 31, 34, 75, 12, 5, 199, 3257, 155] --> ['say', 'jim', 'how', 'about', 'going', 'for', 'a', 'few', 'beers', 'after']\n",
      "[151, 974, 31, 34, 75, 12, 5, 199, 3257, 155, 307] --> ['say', 'jim', 'how', 'about', 'going', 'for', 'a', 'few', 'beers', 'after', 'dinner']\n",
      "[1, 44] --> ['you', 'know']\n",
      "[1, 44, 13] --> ['you', 'know', 'that']\n",
      "[1, 44, 13, 8] --> ['you', 'know', 'that', 'is']\n",
      "[1, 44, 13, 8, 4476] --> ['you', 'know', 'that', 'is', 'tempting']\n",
      "[1, 44, 13, 8, 4476, 26] --> ['you', 'know', 'that', 'is', 'tempting', 'but']\n"
     ]
    }
   ],
   "source": [
    "for sequence in input_sequences[:15]:\n",
    "    print(f'{sequence} --> {[tokenizer.index_word[i] for i in sequence]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1975,
     "status": "ok",
     "timestamp": 1764126235641,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "swO8QDv7Bzld",
    "outputId": "ff02f0d4-4618-4a4d-d809-18d253ab2df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 151, 974], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len))\n",
    "print(len(input_sequences[0]))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1764126243141,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "ntPJXHYgB7UU",
    "outputId": "ea1c3882-7326-41ae-a495-6d8db7ec9866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (989152, 68)\n",
      "Shape of y: (989152,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting in features and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gM-I462qt1g"
   },
   "source": [
    "### **3. Model Architecture (Stacked LSTM)**\n",
    "The model uses a Stacked LSTM architecture:\n",
    "\n",
    "- **Embedding Layer (100-dim):** Learned vector representations of words.\n",
    "\n",
    "- **LSTM Layer 1 (150 units):** Captures lower-level sequence patterns.\n",
    "\n",
    "- **Dropout (0.2):** Prevents overfitting.\n",
    "\n",
    "- **LSTM Layer 2 (100 units):** Captures higher-level semantic context.\n",
    "\n",
    "- **Dense Layer (Softmax):** Predicts the probability of the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 1675,
     "status": "ok",
     "timestamp": 1764126259853,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "6i1atlGdB-yc",
    "outputId": "f3567841-018f-4814-ea6f-c5bb0c42456b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,939,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19391</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,958,491</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │     \u001b[38;5;34m1,939,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m150\u001b[0m)        │       \u001b[38;5;34m150,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m, \u001b[38;5;34m150\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m100,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19391\u001b[0m)          │     \u001b[38;5;34m1,958,491\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,148,591</span> (15.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,148,591\u001b[0m (15.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,148,591</span> (15.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,148,591\u001b[0m (15.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Building LSTM\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_sequence_len-1, )))\n",
    "model.add(Embedding(input_dim=total_words, output_dim=100))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer= 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGggm_UFrSMg"
   },
   "source": [
    "### **4. Training with Checkpointing**\n",
    "To ensure training stability, I employed a manual checkpointing loop. The model is trained in 5-epoch chunks, saving the state to disk after each chunk. This allows for:\n",
    "\n",
    "1. Comparison of model performance at different stages (Epoch 5 vs Epoch 25).\n",
    "\n",
    "2. Fault tolerance against session timeouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722926,
     "status": "ok",
     "timestamp": 1764126993135,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "qTuIY-hICBi0",
    "outputId": "ca2478b9-7da7-4ac6-d56b-2a46c5ce95be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 23ms/step - accuracy: 0.0792 - loss: 6.2945 - val_accuracy: 0.1580 - val_loss: 5.3108\n",
      "Epoch 2/5\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.1643 - loss: 5.1488 - val_accuracy: 0.1833 - val_loss: 5.0605\n",
      "Epoch 3/5\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.1848 - loss: 4.8464 - val_accuracy: 0.1946 - val_loss: 4.9569\n",
      "Epoch 4/5\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.1953 - loss: 4.6691 - val_accuracy: 0.1997 - val_loss: 4.9032\n",
      "Epoch 5/5\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 24ms/step - accuracy: 0.2019 - loss: 4.5441 - val_accuracy: 0.2063 - val_loss: 4.8758\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "history = model.fit(X, y, epochs=5, batch_size=128, validation_split=0.2)\n",
    "model.save('models/model_0_5.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1764127690623,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "rgw8wtrmHqHp"
   },
   "outputs": [],
   "source": [
    "def train_chunk(path, initial):\n",
    "  model = load_model(path)\n",
    "  model.fit(X, y, epochs=initial+5, initial_epoch=initial, batch_size=128, validation_split=0.2)\n",
    "  model.save(f'models/model_{initial}_{initial+5}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 675730,
     "status": "ok",
     "timestamp": 1764128367458,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "_je9kRl3K9RB",
    "outputId": "09e91ca0-c681-4542-81e0-090b6c899e07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.2078 - loss: 4.4498 - val_accuracy: 0.2027 - val_loss: 5.0174\n",
      "Epoch 7/10\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2021 - loss: 4.6968 - val_accuracy: 0.2005 - val_loss: 5.0572\n",
      "Epoch 8/10\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2012 - loss: 4.8370 - val_accuracy: 0.2018 - val_loss: 5.0708\n",
      "Epoch 9/10\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2026 - loss: 4.8878 - val_accuracy: 0.2034 - val_loss: 5.0650\n",
      "Epoch 10/10\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2050 - loss: 4.9042 - val_accuracy: 0.2041 - val_loss: 5.0702\n"
     ]
    }
   ],
   "source": [
    "train_chunk('models/model_0_5.keras', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681521,
     "status": "ok",
     "timestamp": 1764129131114,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "YhzJDZKgLcoZ",
    "outputId": "b021ce3b-fab7-4f5c-9523-2109fffbb46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.2067 - loss: 4.9136 - val_accuracy: 0.2049 - val_loss: 5.0866\n",
      "Epoch 12/15\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2072 - loss: 4.9252 - val_accuracy: 0.2060 - val_loss: 5.0631\n",
      "Epoch 13/15\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2095 - loss: 4.9274 - val_accuracy: 0.2092 - val_loss: 5.0653\n",
      "Epoch 14/15\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2123 - loss: 4.9292 - val_accuracy: 0.2090 - val_loss: 5.0792\n",
      "Epoch 15/15\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 22ms/step - accuracy: 0.2122 - loss: 4.9347 - val_accuracy: 0.2103 - val_loss: 5.0622\n"
     ]
    }
   ],
   "source": [
    "train_chunk('models/model_5_10.keras', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687954,
     "status": "ok",
     "timestamp": 1764129888387,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "D3Rb6sm8P-MQ",
    "outputId": "842e8f08-ea16-4c7c-ab14-a4732db4d5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.2138 - loss: 4.9263 - val_accuracy: 0.2114 - val_loss: 5.0643\n",
      "Epoch 17/20\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2148 - loss: 4.9247 - val_accuracy: 0.2120 - val_loss: 5.0606\n",
      "Epoch 18/20\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.2170 - loss: 4.9252 - val_accuracy: 0.2126 - val_loss: 5.0659\n",
      "Epoch 19/20\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.2178 - loss: 4.9155 - val_accuracy: 0.2118 - val_loss: 5.0855\n",
      "Epoch 20/20\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 22ms/step - accuracy: 0.2178 - loss: 4.9176 - val_accuracy: 0.2124 - val_loss: 5.1060\n"
     ]
    }
   ],
   "source": [
    "train_chunk('models/model_10_15.keras', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 679549,
     "status": "ok",
     "timestamp": 1764130568021,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "Xy8SB4wQS1gI",
    "outputId": "82c3a8e8-77cc-46cb-de45-532167f1fb83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.2185 - loss: 4.9115 - val_accuracy: 0.2139 - val_loss: 5.0919\n",
      "Epoch 22/25\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.2195 - loss: 4.9046 - val_accuracy: 0.2132 - val_loss: 5.1001\n",
      "Epoch 23/25\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2199 - loss: 4.8928 - val_accuracy: 0.2130 - val_loss: 5.0780\n",
      "Epoch 24/25\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2201 - loss: 4.8821 - val_accuracy: 0.2140 - val_loss: 5.1233\n",
      "Epoch 25/25\n",
      "\u001b[1m6183/6183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.2221 - loss: 4.8710 - val_accuracy: 0.2160 - val_loss: 5.0571\n"
     ]
    }
   ],
   "source": [
    "train_chunk('models/model_15_20.keras', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_69klI4Vrlbw"
   },
   "source": [
    "### **5. Model Evaluation & Comparison**\n",
    "We analyze the evolution of the model's intelligence by comparing predictions from different checkpoints.\n",
    "\n",
    "**Epoch 5 (Baby):** Often produces repetitive or grammatical errors.\n",
    "\n",
    "**Epoch 25 (Graduate):** Produces coherent, context-aware sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3814,
     "status": "ok",
     "timestamp": 1764135828066,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "d3WeAKozfu1A",
    "outputId": "7875a305-f1fd-4e77-89a0-3f4a9e715add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING MODEL EVOLUTION:\n",
      "\n",
      "model_0_5.keras prediction: -> 'Hi how are you doing this'\n",
      "model_5_10.keras prediction: -> 'Hi how are you going to'\n",
      "model_10_15.keras prediction: -> 'Hi how are you going to'\n",
      "model_15_20.keras prediction: -> 'Hi how are you going to'\n",
      "model_20_25.keras prediction: -> 'Hi how are you doing today'\n"
     ]
    }
   ],
   "source": [
    "model_files = [\n",
    "    'model_0_5.keras',\n",
    "    'model_5_10.keras',\n",
    "    'model_10_15.keras',\n",
    "    'model_15_20.keras',\n",
    "    'model_20_25.keras'\n",
    "]\n",
    "\n",
    "def generate_text_comparison(model, seed_text, next_words):\n",
    "    output_text = seed_text\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([output_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(probs) # Using Greedy Search for comparison\n",
    "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "        output_text += \" \" + output_word\n",
    "    return output_text\n",
    "\n",
    "print(\"ANALYZING MODEL EVOLUTION:\\n\")\n",
    "test_phrase = \"Hi how are\"\n",
    "\n",
    "for filename in model_files:\n",
    "    try:\n",
    "        m = load_model(f'models/{filename}')\n",
    "        pred = generate_text_comparison(m, test_phrase, 3)\n",
    "        print(f\"{filename} prediction: -> '{pred}'\")\n",
    "        del m\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWwEVn41soa4"
   },
   "source": [
    "### **6. Final Inference Engine**\n",
    "\n",
    "For the production application, we use Temperature Sampling (Top-K / Random Choice) instead of Greedy Search. This introduces variation and prevents the model from getting stuck in repetitive loops like \"how are you how are you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "executionInfo": {
     "elapsed": 4159,
     "status": "ok",
     "timestamp": 1764136396328,
     "user": {
      "displayName": "MJ's Work Space",
      "userId": "02915431501031030320"
     },
     "user_tz": -330
    },
    "id": "HmbV03JuszvI",
    "outputId": "826df787-d41f-457a-d6ae-867a4945efdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Input Text : How are you\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Output: How are you **doing**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_smart(model, text, next_words=1):\n",
    "    max_sequence_len = model.input_shape[1] + 1\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\n",
    "    probs = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "    # Pick randomly from top 3 choices to break loops (Temperature Strategy)\n",
    "    top_indices = probs.argsort()[-3:][::-1]\n",
    "    top_probs = probs[top_indices] / np.sum(probs[top_indices])\n",
    "    predicted_index = np.random.choice(top_indices, p=top_probs)\n",
    "\n",
    "    output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "    return output_word\n",
    "\n",
    "# Load Final Model\n",
    "final_model = load_model('models/model_20_25.keras')\n",
    "\n",
    "# Test\n",
    "from IPython.display import display, Markdown\n",
    "prompt = input('Enter Input Text : ')\n",
    "completion = predict_smart(final_model, prompt, next_words=3)\n",
    "display(Markdown(f\"Output: {prompt} **{completion}**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bso8E0F4s45v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMgekwIsSHIknGrI9fmWSDb",
   "gpuType": "T4",
   "mount_file_id": "16i9ezhPGiR05MFsy7KKh6IoMVFCC7LAV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
